## What is R^2?
- 1 - (sum of squared residuals for predicted Y) / (sum of squared residuals for mean of Y)
- = 1 - Var(fit) / Var(mean)
- proportion of variance explained by the line / predictor 
  - OR 60% reduction of variance when we take [predictor] into account

Note: variance is average sum of squares

## How to estimate R^2
For the denominator, Take the range of y. 

For the numerator, Take the residual of worst data/estimate above the line and worst estimate below the line, find that total. 

Then divide this by the 

## It's possible for R^2 to be negative if model is bad. 

## Alternatives
- adjusted R^2 which penalizes for added variables (otherwise R^2 can increase with silly variables)

## P-value (how reliable the relationship is)
We need to see if R^2 is significant because an R^2 of 1 doesn't necessarily mean it is. (a line for 2 points can get a R^2 of 1). So we use p-value.

F = variation in Y explained by X / variation in Y not explained by X
In terms of SS, F is also defined in terms of degrees of freedom. 

If fit is good, then F should be big. Using F distribution generated from degrees of freedom, we can calculate p-value to see whether the R^2 is significant. 

## Sources
- StatQuest: Linear Models Pt.1 - Linear Regression: https://www.youtube.com/watch?v=nk2CQITm_eo
